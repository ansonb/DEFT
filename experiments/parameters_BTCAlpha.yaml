data: bitcoinalpha  # bitcoinalpha bitcoinotc

bitcoinalpha_args:
  folder: ./data/
  edges_file: soc-sign-bitcoinalpha.csv
  aggr_time: 1200000 #three weeks in seconds: 1200000
  feats_per_node: 3

use_cuda: True
use_logfile: True

model: DEFT

task: edge_cls # link_pred edge_cls

class_weights: [ 0.8, 0.2]
use_2_hot_node_feats: False
use_1_hot_node_feats: True
save_node_embeddings: False

train_proportion: 0.7
dev_proportion: 0.1

num_epochs: 1000 #number of passes though the data
steps_accum_gradients: 1
learning_rate: 0.001
learning_rate_min: 0.0001
learning_rate_max: 0.1
negative_mult_training: 20
negative_mult_test: 100
smart_neg_sampling: True
seed: 1234
target_measure: F1 # measure to define the best epoch F1, Precision, Recall, MRR, MAP, Loss
target_class: AVG # Target class to get the measure to define the best epoch (AVG, 0, 1)
early_stop_patience: 100

eval_after_epochs: 5
adj_mat_time_window: 1
num_hist_steps: 10 # number of previous steps used for prediction
num_hist_steps_min: 3 # only used if num_hist_steps: None
num_hist_steps_max: 10 # only used if num_hist_steps: None

data_loading_params:
  batch_size: 1
  num_workers: 8

gcn_parameters:
  feats_per_node: 100
  feats_per_node_min: 50
  feats_per_node_max: 256
  layer_1_feats: 128
  layer_1_feats_min: 20
  layer_1_feats_max: 200
  layer_2_feats: 128
  layer_2_feats_same_as_l1: True
  k_top_grcu: 200
  num_layers: 2
  lstm_l1_layers: 1
  lstm_l1_feats: 68 # only used with sp_lstm_B_trainer
  lstm_l1_feats_min: 20
  lstm_l1_feats_max: 200
  lstm_l2_layers: 1 # only used with both sp_lstm_A_trainer and sp_lstm_B_trainer
  lstm_l2_feats: None # only used with both sp_lstm_A_trainer and sp_lstm_B_trainer
  lstm_l2_feats_same_as_l1: True
  cls_feats: 77 # Hidden size of the classifier
  cls_feats_min: 50
  cls_feats_max: 500
comments:
  - comments
transformer_parameters:
  in_dim: 100 # node_dim (feat is an integer)
  hidden_dim: 128
  out_dim: 128
  n_classes: 2
  n_heads: 8
  in_feat_dropout: 0.0
  dropout: 0.0
  L: 4
  readout: mean
  layer_norm: False
  batch_norm: True
  residual: True
  lap_pos_enc: False
  wl_pos_enc: False
  pos_enc_dim: 8
  full_graph: False 
  out_feats1: 128 # Not being used
  filter_order: 4
  in_channels_sgnn: 128 # Not being used
  out_channels_sgnn: 128
  fc1_dim: 128
  pe_dim: 128
  out_feats: 128
  num_heads: 8
  layer_norm: False
  batch_norm: True
  is_recurrent: False
  sgwt_scales: [0.1,1,10]
  use_transformer: True
  concat_in_skipfeat: False
  rt_residual: True
  skip_in_feat: True
  use_spatial_feat_in_lpe: True
  use_spatial_feat_in_rgt_ip: True
  skip_rgt_in_feat: False